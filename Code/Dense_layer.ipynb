{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dadc7002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 12:02:14.310004: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-12 12:02:14.322026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-12 12:02:14.335814: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-12 12:02:14.339900: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-12 12:02:14.350474: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-12 12:02:16.297233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import  models\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from sentence_transformers import SentenceTransformer,  models\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8154e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sent_Embed_Dense(nn.Module):\n",
    "    def __init__(self, model_name, pooling_mode, input_dim, output_dim=1):\n",
    "        super(Sent_Embed_Dense, self).__init__()\n",
    "        self.sbert = SentenceTransformer(model_name)\n",
    "        self.pooling = models.Pooling(self.sbert.get_sentence_embedding_dimension(), pooling_mode)\n",
    "        self.dense = nn.Linear(2 * input_dim, output_dim)  # input_dim = input_dim for element wise subtraction\n",
    "    \n",
    "    def forward(self, features):\n",
    "        text1 = features['text1']\n",
    "        text2 = features['text2']\n",
    "\n",
    "        # Get embeddings for both texts\n",
    "        embeddings1 = self.sbert.encode(text1, convert_to_tensor=True)\n",
    "        embeddings2 = self.sbert.encode(text2, convert_to_tensor=True)\n",
    "\n",
    "        # Concatenate embeddings\n",
    "        combined_embeddings = torch.concat((embeddings1, embeddings2), dim=1)\n",
    "\n",
    "        # element wise subtraction embeddings\n",
    "        #combined_embeddings = (embeddings1- embeddings2)\n",
    "        output = self.dense(combined_embeddings)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, text1, text2, labels):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2\n",
    "        self.labels = labels\n",
    " \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text1 = self.text1[idx]\n",
    "        text2 = self.text2[idx]\n",
    "        label = self.labels[idx]\n",
    "        return {'text1': text1, 'text2': text2}, torch.tensor(label, dtype=torch.float32)\n",
    "    \n",
    "def make_dataloader(df,batch_size):\n",
    "    texts1 = df['user_thoughts_and _feelings'].values\n",
    "    texts2 = df['designer_guess'].values\n",
    "    labels = df['Avg_EA'].values\n",
    "    \n",
    "    dataset = CustomDataset(texts1, texts2, labels)\n",
    "    dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size, collate_fn=custom_collate_fn)\n",
    "    #print(zip(*dataloader))\n",
    "    return dataloader\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    combined_inputs = {}\n",
    "    for key in texts[0].keys():\n",
    "        combined_inputs[key] = [text[key] for text in texts]\n",
    "    \n",
    "    # Convert labels to tensors and move to GPU\n",
    "    labels = (labels)\n",
    "    \n",
    "    return combined_inputs, labels\n",
    "\n",
    "def calculate_correlation(predictions, labels):\n",
    "    # Flatten tensors and convert to numpy arrays\n",
    "    predictions = torch.cat(predictions).cpu().detach().numpy().flatten()\n",
    "    labels = torch.cat(labels).cpu().detach().numpy().flatten()\n",
    "    #print(predictions)\n",
    "    \n",
    "    if len(predictions) < 2 or len(labels) < 2:\n",
    "        raise ValueError(\"Both predictions and labels must have at least 2 data points\")\n",
    "    \n",
    "    # Compute Pearson correlation\n",
    "    pearson, _ = pearsonr(predictions, labels)  \n",
    "    rmse = np.sqrt(np.mean((labels - predictions)**2))\n",
    "    spearman, _ = spearmanr(predictions, labels)\n",
    "    return pearson,spearman, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53176b56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ofabunmi/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1, Val Loss: 0.0841490375453658\n",
      "Fold 1, Epoch 2, Val Loss: 0.08341630381918574\n",
      "Fold 1, Epoch 3, Val Loss: 0.08320680377565977\n",
      "Fold 1, Epoch 4, Val Loss: 0.08361910475745431\n",
      "Fold 1, Epoch 5, Val Loss: 0.08329329764237628\n",
      "Fold 1, Epoch 6, Val Loss: 0.0836639200513471\n",
      "Fold 1, Epoch 7, Val Loss: 0.0838973924349476\n",
      "Fold 1, Epoch 8, Val Loss: 0.08370037173380196\n",
      "Fold 1, Epoch 9, Val Loss: 0.08342840529088183\n",
      "Fold 1, Epoch 10, Val Loss: 0.08456017305208105\n",
      "Fold 1, Epoch 11, Val Loss: 0.08451349695785919\n",
      "Fold 1, Epoch 12, Val Loss: 0.08519762037762525\n",
      "Fold 1, Epoch 13, Val Loss: 0.0851822223226514\n",
      "Fold 1, Epoch 14, Val Loss: 0.08519039881745509\n",
      "Fold 1, Epoch 15, Val Loss: 0.08647027837076975\n",
      "Fold 1, Epoch 16, Val Loss: 0.08637963502163377\n",
      "Fold 1, Epoch 17, Val Loss: 0.08717418283793248\n",
      "Fold 1, Epoch 18, Val Loss: 0.08724933824269102\n",
      "Fold 1, Epoch 19, Val Loss: 0.08854503751693604\n",
      "Fold 1, Epoch 20, Val Loss: 0.08846234194481642\n",
      "Fold 1, Epoch 21, Val Loss: 0.08827874321205956\n",
      "Fold 1, Epoch 22, Val Loss: 0.08885386045989839\n",
      "Fold 1, Epoch 23, Val Loss: 0.08896997092136492\n",
      "Fold 1, Epoch 24, Val Loss: 0.08988852960400336\n",
      "Fold 1, Epoch 25, Val Loss: 0.0901044550051059\n",
      "Fold 1, Epoch 26, Val Loss: 0.09083241151350797\n",
      "Fold 1, Epoch 27, Val Loss: 0.091104591978688\n",
      "Fold 1, Epoch 28, Val Loss: 0.09180779620017145\n",
      "Fold 1, Epoch 29, Val Loss: 0.09194126363745093\n",
      "Fold 1, Epoch 30, Val Loss: 0.09235025326524414\n",
      "Fold 1, Epoch 31, Val Loss: 0.0923892121855109\n",
      "Fold 1, Epoch 32, Val Loss: 0.09316584975119516\n",
      "Fold 1, Epoch 33, Val Loss: 0.09316372901623898\n",
      "Fold 1, Epoch 34, Val Loss: 0.09347800273045424\n",
      "Fold 1, Epoch 35, Val Loss: 0.09379797571035688\n",
      "Fold 1, Epoch 36, Val Loss: 0.09416266070085032\n",
      "Fold 1, Epoch 37, Val Loss: 0.09527389832193067\n",
      "Fold 1, Epoch 38, Val Loss: 0.09601246413261227\n",
      "Fold 1, Epoch 39, Val Loss: 0.09584279149943944\n",
      "Fold 1, Epoch 40, Val Loss: 0.09645708767303506\n",
      "Fold 1, Epoch 41, Val Loss: 0.09675234884299344\n",
      "Fold 1, Epoch 42, Val Loss: 0.09667724336154607\n",
      "Fold 1, Epoch 43, Val Loss: 0.09761999921405125\n",
      "Fold 1, Epoch 44, Val Loss: 0.09765796354040504\n",
      "Fold 1, Epoch 45, Val Loss: 0.09781310377098577\n",
      "Fold 1, Epoch 46, Val Loss: 0.09794397222399817\n",
      "Fold 1, Epoch 47, Val Loss: 0.09839748376686551\n",
      "Fold 1, Epoch 48, Val Loss: 0.09863342776839595\n",
      "Fold 1, Epoch 49, Val Loss: 0.09973370110008849\n",
      "Fold 1, Epoch 50, Val Loss: 0.09934800549634196\n",
      "Fold 2, Epoch 1, Val Loss: 0.05668362285815347\n",
      "Fold 2, Epoch 2, Val Loss: 0.058255349198346566\n",
      "Fold 2, Epoch 3, Val Loss: 0.0610844952012725\n",
      "Fold 2, Epoch 4, Val Loss: 0.062337766550192304\n",
      "Fold 2, Epoch 5, Val Loss: 0.06388872698917111\n",
      "Fold 2, Epoch 6, Val Loss: 0.06684334901040327\n",
      "Fold 2, Epoch 7, Val Loss: 0.06688433563255229\n",
      "Fold 2, Epoch 8, Val Loss: 0.06827340462920499\n",
      "Fold 2, Epoch 9, Val Loss: 0.07031468613054635\n",
      "Fold 2, Epoch 10, Val Loss: 0.07051520415621881\n",
      "Fold 2, Epoch 11, Val Loss: 0.07214622242093659\n",
      "Fold 2, Epoch 12, Val Loss: 0.07366221989365537\n",
      "Fold 2, Epoch 13, Val Loss: 0.07354653016938768\n",
      "Fold 2, Epoch 14, Val Loss: 0.07465667706912629\n",
      "Fold 2, Epoch 15, Val Loss: 0.07594828732136294\n",
      "Fold 2, Epoch 16, Val Loss: 0.07696781119581525\n",
      "Fold 2, Epoch 17, Val Loss: 0.0774541874792097\n",
      "Fold 2, Epoch 18, Val Loss: 0.07950571542508746\n",
      "Fold 2, Epoch 19, Val Loss: 0.0792384556792159\n",
      "Fold 2, Epoch 20, Val Loss: 0.07923728351630012\n",
      "Fold 2, Epoch 21, Val Loss: 0.08055059962045763\n",
      "Fold 2, Epoch 22, Val Loss: 0.08152800151599675\n",
      "Fold 2, Epoch 23, Val Loss: 0.08306215392063274\n",
      "Fold 2, Epoch 24, Val Loss: 0.08242573611334794\n",
      "Fold 2, Epoch 25, Val Loss: 0.08318727910292283\n",
      "Fold 2, Epoch 26, Val Loss: 0.08400775403198268\n",
      "Fold 2, Epoch 27, Val Loss: 0.08465233525364763\n",
      "Fold 2, Epoch 28, Val Loss: 0.08557977033746687\n",
      "Fold 2, Epoch 29, Val Loss: 0.08591405708252245\n",
      "Fold 2, Epoch 30, Val Loss: 0.08647354565780713\n",
      "Fold 2, Epoch 31, Val Loss: 0.08749121267173905\n",
      "Fold 2, Epoch 32, Val Loss: 0.08851960189385863\n",
      "Fold 2, Epoch 33, Val Loss: 0.08838976454668833\n",
      "Fold 2, Epoch 34, Val Loss: 0.08845147798807981\n",
      "Fold 2, Epoch 35, Val Loss: 0.08938994166383055\n",
      "Fold 2, Epoch 36, Val Loss: 0.0896822174228469\n",
      "Fold 2, Epoch 37, Val Loss: 0.0903592682029638\n",
      "Fold 2, Epoch 38, Val Loss: 0.09161655806852245\n",
      "Fold 2, Epoch 39, Val Loss: 0.0916694407593847\n",
      "Fold 2, Epoch 40, Val Loss: 0.0927152338571937\n",
      "Fold 2, Epoch 41, Val Loss: 0.0920620076390656\n",
      "Fold 2, Epoch 42, Val Loss: 0.09257284653075556\n",
      "Fold 2, Epoch 43, Val Loss: 0.09275148148428546\n",
      "Fold 2, Epoch 44, Val Loss: 0.0934893741243286\n",
      "Fold 2, Epoch 45, Val Loss: 0.09372475363003711\n",
      "Fold 2, Epoch 46, Val Loss: 0.09518608278182607\n",
      "Fold 2, Epoch 47, Val Loss: 0.09500358530583779\n",
      "Fold 2, Epoch 48, Val Loss: 0.09526702283037594\n",
      "Fold 2, Epoch 49, Val Loss: 0.09593402601264339\n",
      "Fold 2, Epoch 50, Val Loss: 0.09686714504423183\n",
      "Fold 3, Epoch 1, Val Loss: 0.03335109092988407\n",
      "Fold 3, Epoch 2, Val Loss: 0.034533134921649535\n",
      "Fold 3, Epoch 3, Val Loss: 0.035641712597382344\n",
      "Fold 3, Epoch 4, Val Loss: 0.036826879746594286\n",
      "Fold 3, Epoch 5, Val Loss: 0.037899647209107774\n",
      "Fold 3, Epoch 6, Val Loss: 0.03892769738922652\n",
      "Fold 3, Epoch 7, Val Loss: 0.039669212921580765\n",
      "Fold 3, Epoch 8, Val Loss: 0.04003256701422894\n",
      "Fold 3, Epoch 9, Val Loss: 0.041491533148251214\n",
      "Fold 3, Epoch 10, Val Loss: 0.041812940338826994\n",
      "Fold 3, Epoch 11, Val Loss: 0.042550576263844654\n",
      "Fold 3, Epoch 12, Val Loss: 0.04304779834217495\n",
      "Fold 3, Epoch 13, Val Loss: 0.043709016701884845\n",
      "Fold 3, Epoch 14, Val Loss: 0.04430921150400688\n",
      "Fold 3, Epoch 15, Val Loss: 0.04534196500343973\n",
      "Fold 3, Epoch 16, Val Loss: 0.04605352347540854\n",
      "Fold 3, Epoch 17, Val Loss: 0.04664601460042629\n",
      "Fold 3, Epoch 18, Val Loss: 0.0470170928490562\n",
      "Fold 3, Epoch 19, Val Loss: 0.04807439887353237\n",
      "Fold 3, Epoch 20, Val Loss: 0.048697853771951016\n",
      "Fold 3, Epoch 21, Val Loss: 0.04946819609853416\n",
      "Fold 3, Epoch 22, Val Loss: 0.04956983535699288\n",
      "Fold 3, Epoch 23, Val Loss: 0.05012001060218204\n",
      "Fold 3, Epoch 24, Val Loss: 0.05094776064434225\n",
      "Fold 3, Epoch 25, Val Loss: 0.05133200256674652\n",
      "Fold 3, Epoch 26, Val Loss: 0.05233439561150703\n",
      "Fold 3, Epoch 27, Val Loss: 0.05298602670357771\n",
      "Fold 3, Epoch 28, Val Loss: 0.0539909199341941\n",
      "Fold 3, Epoch 29, Val Loss: 0.05454607114566493\n",
      "Fold 3, Epoch 30, Val Loss: 0.05455439748648182\n",
      "Fold 3, Epoch 31, Val Loss: 0.05610159586827245\n",
      "Fold 3, Epoch 32, Val Loss: 0.05586502559563087\n",
      "Fold 3, Epoch 33, Val Loss: 0.05628698413143462\n",
      "Fold 3, Epoch 34, Val Loss: 0.05692970925530921\n",
      "Fold 3, Epoch 35, Val Loss: 0.05764372709801301\n",
      "Fold 3, Epoch 36, Val Loss: 0.05815418274254651\n",
      "Fold 3, Epoch 37, Val Loss: 0.05871660366837103\n",
      "Fold 3, Epoch 38, Val Loss: 0.059551812628706104\n",
      "Fold 3, Epoch 39, Val Loss: 0.059722024375959235\n",
      "Fold 3, Epoch 40, Val Loss: 0.06034689735771697\n",
      "Fold 3, Epoch 41, Val Loss: 0.060720615144691695\n",
      "Fold 3, Epoch 42, Val Loss: 0.061220420409032764\n",
      "Fold 3, Epoch 43, Val Loss: 0.06258371074614438\n",
      "Fold 3, Epoch 44, Val Loss: 0.062225712815901106\n",
      "Fold 3, Epoch 45, Val Loss: 0.06300481839623381\n",
      "Fold 3, Epoch 46, Val Loss: 0.06341870604749905\n",
      "Fold 3, Epoch 47, Val Loss: 0.06406784848468558\n",
      "Fold 3, Epoch 48, Val Loss: 0.06414228356540358\n",
      "Fold 3, Epoch 49, Val Loss: 0.06472184989963554\n",
      "Fold 3, Epoch 50, Val Loss: 0.0651273080144266\n",
      "Fold 4, Epoch 1, Val Loss: 0.01219912868914848\n",
      "Fold 4, Epoch 2, Val Loss: 0.01417492139070766\n",
      "Fold 4, Epoch 3, Val Loss: 0.014434966256224223\n",
      "Fold 4, Epoch 4, Val Loss: 0.016791218544972706\n",
      "Fold 4, Epoch 5, Val Loss: 0.016031519999242762\n",
      "Fold 4, Epoch 6, Val Loss: 0.016897519002131132\n",
      "Fold 4, Epoch 7, Val Loss: 0.01877983568133924\n",
      "Fold 4, Epoch 8, Val Loss: 0.018143473540311043\n",
      "Fold 4, Epoch 9, Val Loss: 0.017905643058342816\n",
      "Fold 4, Epoch 10, Val Loss: 0.01905484574110067\n",
      "Fold 4, Epoch 11, Val Loss: 0.02006461209913885\n",
      "Fold 4, Epoch 12, Val Loss: 0.021391572236242693\n",
      "Fold 4, Epoch 13, Val Loss: 0.0208943954113982\n",
      "Fold 4, Epoch 14, Val Loss: 0.02297584690630701\n",
      "Fold 4, Epoch 15, Val Loss: 0.023206850529412293\n",
      "Fold 4, Epoch 16, Val Loss: 0.02285278700534186\n",
      "Fold 4, Epoch 17, Val Loss: 0.02275469942576985\n",
      "Fold 4, Epoch 18, Val Loss: 0.02369375142561384\n",
      "Fold 4, Epoch 19, Val Loss: 0.025251936191310085\n",
      "Fold 4, Epoch 20, Val Loss: 0.0249193517738604\n",
      "Fold 4, Epoch 21, Val Loss: 0.025497669792085533\n",
      "Fold 4, Epoch 22, Val Loss: 0.025977101277021575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 23, Val Loss: 0.02672941023105192\n",
      "Fold 4, Epoch 24, Val Loss: 0.027842554444620502\n",
      "Fold 4, Epoch 25, Val Loss: 0.027285002978136286\n",
      "Fold 4, Epoch 26, Val Loss: 0.028837831689078153\n",
      "Fold 4, Epoch 27, Val Loss: 0.029337229482073533\n",
      "Fold 4, Epoch 28, Val Loss: 0.029833091203148466\n",
      "Fold 4, Epoch 29, Val Loss: 0.030130797817181785\n",
      "Fold 4, Epoch 30, Val Loss: 0.02993427580270882\n",
      "Fold 4, Epoch 31, Val Loss: 0.030807317675473976\n",
      "Fold 4, Epoch 32, Val Loss: 0.030093618380233653\n",
      "Fold 4, Epoch 33, Val Loss: 0.03174664764165672\n",
      "Fold 4, Epoch 34, Val Loss: 0.032069495655191946\n",
      "Fold 4, Epoch 35, Val Loss: 0.03214895934347662\n",
      "Fold 4, Epoch 36, Val Loss: 0.03147570405185847\n",
      "Fold 4, Epoch 37, Val Loss: 0.03378253420198069\n",
      "Fold 4, Epoch 38, Val Loss: 0.03568193331582072\n",
      "Fold 4, Epoch 39, Val Loss: 0.03415493198516825\n",
      "Fold 4, Epoch 40, Val Loss: 0.034756638277837155\n",
      "Fold 4, Epoch 41, Val Loss: 0.03476056904393555\n",
      "Fold 4, Epoch 42, Val Loss: 0.03682978931614012\n",
      "Fold 4, Epoch 43, Val Loss: 0.03609340741100456\n",
      "Fold 4, Epoch 44, Val Loss: 0.03653047983168666\n",
      "Fold 4, Epoch 45, Val Loss: 0.0390171176463396\n",
      "Fold 4, Epoch 46, Val Loss: 0.037235980370548365\n",
      "Fold 4, Epoch 47, Val Loss: 0.037682986520787214\n",
      "Fold 4, Epoch 48, Val Loss: 0.037321028844980605\n",
      "Fold 4, Epoch 49, Val Loss: 0.03810579334396442\n",
      "Fold 4, Epoch 50, Val Loss: 0.03799107653081502\n",
      "Fold 5, Epoch 1, Val Loss: 0.01588442484560398\n",
      "Fold 5, Epoch 2, Val Loss: 0.017042488018804845\n",
      "Fold 5, Epoch 3, Val Loss: 0.018334986486190082\n",
      "Fold 5, Epoch 4, Val Loss: 0.01896739286297072\n",
      "Fold 5, Epoch 5, Val Loss: 0.020235457337693417\n",
      "Fold 5, Epoch 6, Val Loss: 0.020987707737566034\n",
      "Fold 5, Epoch 7, Val Loss: 0.021773296990431845\n",
      "Fold 5, Epoch 8, Val Loss: 0.022700077418645295\n",
      "Fold 5, Epoch 9, Val Loss: 0.02357659835315541\n",
      "Fold 5, Epoch 10, Val Loss: 0.02418981746487506\n",
      "Fold 5, Epoch 11, Val Loss: 0.024569369528390696\n",
      "Fold 5, Epoch 12, Val Loss: 0.025424560640714654\n",
      "Fold 5, Epoch 13, Val Loss: 0.026141579139153815\n",
      "Fold 5, Epoch 14, Val Loss: 0.026702211814567403\n",
      "Fold 5, Epoch 15, Val Loss: 0.02733917851268912\n",
      "Fold 5, Epoch 16, Val Loss: 0.027714068189445905\n",
      "Fold 5, Epoch 17, Val Loss: 0.02845614060776246\n",
      "Fold 5, Epoch 18, Val Loss: 0.029169471702796136\n",
      "Fold 5, Epoch 19, Val Loss: 0.029494970207281635\n",
      "Fold 5, Epoch 20, Val Loss: 0.029892427795361567\n",
      "Fold 5, Epoch 21, Val Loss: 0.03062433623968404\n",
      "Fold 5, Epoch 22, Val Loss: 0.03145856305772296\n",
      "Fold 5, Epoch 23, Val Loss: 0.03174281210737139\n",
      "Fold 5, Epoch 24, Val Loss: 0.0324893820552616\n",
      "Fold 5, Epoch 25, Val Loss: 0.032839673575422654\n",
      "Fold 5, Epoch 26, Val Loss: 0.03330475214852413\n",
      "Fold 5, Epoch 27, Val Loss: 0.0340346952041066\n",
      "Fold 5, Epoch 28, Val Loss: 0.034714682190672023\n",
      "Fold 5, Epoch 29, Val Loss: 0.034751933550069225\n",
      "Fold 5, Epoch 30, Val Loss: 0.03541410779904585\n",
      "Fold 5, Epoch 31, Val Loss: 0.0357310282663341\n",
      "Fold 5, Epoch 32, Val Loss: 0.036596778886240085\n",
      "Fold 5, Epoch 33, Val Loss: 0.03707655281798659\n",
      "Fold 5, Epoch 34, Val Loss: 0.03754846190614361\n",
      "Fold 5, Epoch 35, Val Loss: 0.03817692378098317\n",
      "Fold 5, Epoch 36, Val Loss: 0.03838392821845702\n",
      "Fold 5, Epoch 37, Val Loss: 0.038860793285328495\n",
      "Fold 5, Epoch 38, Val Loss: 0.03939609756807014\n",
      "Fold 5, Epoch 39, Val Loss: 0.04007512310427459\n",
      "Fold 5, Epoch 40, Val Loss: 0.04099582789196645\n",
      "Fold 5, Epoch 41, Val Loss: 0.04123348795358404\n",
      "Fold 5, Epoch 42, Val Loss: 0.04147663663570711\n",
      "Fold 5, Epoch 43, Val Loss: 0.041562700099120775\n",
      "Fold 5, Epoch 44, Val Loss: 0.04182341965034456\n",
      "Fold 5, Epoch 45, Val Loss: 0.04278521969660109\n",
      "Fold 5, Epoch 46, Val Loss: 0.043357966864001375\n",
      "Fold 5, Epoch 47, Val Loss: 0.04363918280660073\n",
      "Fold 5, Epoch 48, Val Loss: 0.044078959510794775\n",
      "Fold 5, Epoch 49, Val Loss: 0.044442161642254605\n",
      "Fold 5, Epoch 50, Val Loss: 0.04478115688371789\n",
      "Fold 6, Epoch 1, Val Loss: 0.01351798679148818\n",
      "Fold 6, Epoch 2, Val Loss: 0.014067932818808127\n",
      "Fold 6, Epoch 3, Val Loss: 0.0155517153934232\n",
      "Fold 6, Epoch 4, Val Loss: 0.015259610486199645\n",
      "Fold 6, Epoch 5, Val Loss: 0.01611622341740506\n",
      "Fold 6, Epoch 6, Val Loss: 0.017278676268564264\n",
      "Fold 6, Epoch 7, Val Loss: 0.018307723801210717\n",
      "Fold 6, Epoch 8, Val Loss: 0.018519169632358255\n",
      "Fold 6, Epoch 9, Val Loss: 0.018619769230016656\n",
      "Fold 6, Epoch 10, Val Loss: 0.019103750433714595\n",
      "Fold 6, Epoch 11, Val Loss: 0.01952964747231983\n",
      "Fold 6, Epoch 12, Val Loss: 0.020659837447469312\n",
      "Fold 6, Epoch 13, Val Loss: 0.02040969054194813\n",
      "Fold 6, Epoch 14, Val Loss: 0.021766378776801917\n",
      "Fold 6, Epoch 15, Val Loss: 0.022176121103122816\n",
      "Fold 6, Epoch 16, Val Loss: 0.02169869628439549\n",
      "Fold 6, Epoch 17, Val Loss: 0.02318915653330061\n",
      "Fold 6, Epoch 18, Val Loss: 0.02350788446140293\n",
      "Fold 6, Epoch 19, Val Loss: 0.022384651862508195\n",
      "Fold 6, Epoch 20, Val Loss: 0.024627108846289048\n",
      "Fold 6, Epoch 21, Val Loss: 0.02448080250612541\n",
      "Fold 6, Epoch 22, Val Loss: 0.02499935643446737\n",
      "Fold 6, Epoch 23, Val Loss: 0.025333590829301433\n",
      "Fold 6, Epoch 24, Val Loss: 0.026289235497270882\n",
      "Fold 6, Epoch 25, Val Loss: 0.026020456258215384\n",
      "Fold 6, Epoch 26, Val Loss: 0.02635838003086829\n",
      "Fold 6, Epoch 27, Val Loss: 0.02735003897445991\n",
      "Fold 6, Epoch 28, Val Loss: 0.028745867639816656\n",
      "Fold 6, Epoch 29, Val Loss: 0.02824721992915742\n",
      "Fold 6, Epoch 30, Val Loss: 0.029461468929511625\n",
      "Fold 6, Epoch 31, Val Loss: 0.029957942666559812\n",
      "Fold 6, Epoch 32, Val Loss: 0.030047929886884296\n",
      "Fold 6, Epoch 33, Val Loss: 0.02992945958803003\n",
      "Fold 6, Epoch 34, Val Loss: 0.0302592689497694\n",
      "Fold 6, Epoch 35, Val Loss: 0.031330041914493734\n",
      "Fold 6, Epoch 36, Val Loss: 0.030103087349712867\n",
      "Fold 6, Epoch 37, Val Loss: 0.030657530626988526\n",
      "Fold 6, Epoch 38, Val Loss: 0.03250077646248816\n",
      "Fold 6, Epoch 39, Val Loss: 0.03292551681625062\n",
      "Fold 6, Epoch 40, Val Loss: 0.03291250407888087\n",
      "Fold 6, Epoch 41, Val Loss: 0.03517722079864556\n",
      "Fold 6, Epoch 42, Val Loss: 0.03335165939361931\n",
      "Fold 6, Epoch 43, Val Loss: 0.03475000429542971\n",
      "Fold 6, Epoch 44, Val Loss: 0.03459917380600204\n",
      "Fold 6, Epoch 45, Val Loss: 0.035089341488452934\n",
      "Fold 6, Epoch 46, Val Loss: 0.03578975741072296\n",
      "Fold 6, Epoch 47, Val Loss: 0.03640315207494185\n",
      "Fold 6, Epoch 48, Val Loss: 0.036421191366599714\n",
      "Fold 6, Epoch 49, Val Loss: 0.03622380064679088\n",
      "Fold 6, Epoch 50, Val Loss: 0.037340342764351095\n",
      "Fold 7, Epoch 1, Val Loss: 0.010993564885128844\n",
      "Fold 7, Epoch 2, Val Loss: 0.012518573729005505\n",
      "Fold 7, Epoch 3, Val Loss: 0.012221163586382356\n",
      "Fold 7, Epoch 4, Val Loss: 0.011907773147469087\n",
      "Fold 7, Epoch 5, Val Loss: 0.012417276853170733\n",
      "Fold 7, Epoch 6, Val Loss: 0.013368871037533642\n",
      "Fold 7, Epoch 7, Val Loss: 0.013220877493366746\n",
      "Fold 7, Epoch 8, Val Loss: 0.01547107660392364\n",
      "Fold 7, Epoch 9, Val Loss: 0.014514261392540882\n",
      "Fold 7, Epoch 10, Val Loss: 0.016039385616770895\n",
      "Fold 7, Epoch 11, Val Loss: 0.015647834547318128\n",
      "Fold 7, Epoch 12, Val Loss: 0.015004166547607218\n",
      "Fold 7, Epoch 13, Val Loss: 0.015475305099848603\n",
      "Fold 7, Epoch 14, Val Loss: 0.015622380279852037\n",
      "Fold 7, Epoch 15, Val Loss: 0.01634924877953575\n",
      "Fold 7, Epoch 16, Val Loss: 0.017094193807065228\n",
      "Fold 7, Epoch 17, Val Loss: 0.01681463720683839\n",
      "Fold 7, Epoch 18, Val Loss: 0.018100235448288062\n",
      "Fold 7, Epoch 19, Val Loss: 0.01847477331815319\n",
      "Fold 7, Epoch 20, Val Loss: 0.018255549821023207\n",
      "Fold 7, Epoch 21, Val Loss: 0.018383677566279486\n",
      "Fold 7, Epoch 22, Val Loss: 0.0191452519389114\n",
      "Fold 7, Epoch 23, Val Loss: 0.019345204757955384\n",
      "Fold 7, Epoch 24, Val Loss: 0.01994731224234866\n",
      "Fold 7, Epoch 25, Val Loss: 0.020691300534984046\n",
      "Fold 7, Epoch 26, Val Loss: 0.021203546430652667\n",
      "Fold 7, Epoch 27, Val Loss: 0.020563785696987743\n",
      "Fold 7, Epoch 28, Val Loss: 0.021183169460029336\n",
      "Fold 7, Epoch 29, Val Loss: 0.022209544967112874\n",
      "Fold 7, Epoch 30, Val Loss: 0.02368483307362691\n",
      "Fold 7, Epoch 31, Val Loss: 0.023152161777116733\n",
      "Fold 7, Epoch 32, Val Loss: 0.023498862261807186\n",
      "Fold 7, Epoch 33, Val Loss: 0.023077737786304005\n",
      "Fold 7, Epoch 34, Val Loss: 0.025285171295959943\n",
      "Fold 7, Epoch 35, Val Loss: 0.02437909075392368\n",
      "Fold 7, Epoch 36, Val Loss: 0.021980727042401584\n",
      "Fold 7, Epoch 37, Val Loss: 0.024870149317890042\n",
      "Fold 7, Epoch 38, Val Loss: 0.023620074123217902\n",
      "Fold 7, Epoch 39, Val Loss: 0.024746551101646797\n",
      "Fold 7, Epoch 40, Val Loss: 0.02456951254438102\n",
      "Fold 7, Epoch 41, Val Loss: 0.023645191903349107\n",
      "Fold 7, Epoch 42, Val Loss: 0.02565419848902324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7, Epoch 43, Val Loss: 0.025781625691892754\n",
      "Fold 7, Epoch 44, Val Loss: 0.02688139560979669\n",
      "Fold 7, Epoch 45, Val Loss: 0.026044412319565294\n",
      "Fold 7, Epoch 46, Val Loss: 0.026164974194326597\n",
      "Fold 7, Epoch 47, Val Loss: 0.026982840643974916\n",
      "Fold 7, Epoch 48, Val Loss: 0.02828281737079537\n",
      "Fold 7, Epoch 49, Val Loss: 0.027851185242401367\n",
      "Fold 7, Epoch 50, Val Loss: 0.027260894255425164\n",
      "Fold 8, Epoch 1, Val Loss: 0.00579370520339172\n",
      "Fold 8, Epoch 2, Val Loss: 0.005444158017326117\n",
      "Fold 8, Epoch 3, Val Loss: 0.006710628563144961\n",
      "Fold 8, Epoch 4, Val Loss: 0.007053612569011007\n",
      "Fold 8, Epoch 5, Val Loss: 0.0074171345044801865\n",
      "Fold 8, Epoch 6, Val Loss: 0.008097666746008063\n",
      "Fold 8, Epoch 7, Val Loss: 0.007389459457372864\n",
      "Fold 8, Epoch 8, Val Loss: 0.00858183106515753\n",
      "Fold 8, Epoch 9, Val Loss: 0.00891250876958855\n",
      "Fold 8, Epoch 10, Val Loss: 0.00880386626985538\n",
      "Fold 8, Epoch 11, Val Loss: 0.009000009279506312\n",
      "Fold 8, Epoch 12, Val Loss: 0.008946404186396896\n",
      "Fold 8, Epoch 13, Val Loss: 0.009899978445570223\n",
      "Fold 8, Epoch 14, Val Loss: 0.010651750015456556\n",
      "Fold 8, Epoch 15, Val Loss: 0.010888039699692148\n",
      "Fold 8, Epoch 16, Val Loss: 0.011097980270402662\n",
      "Fold 8, Epoch 17, Val Loss: 0.010692972166951491\n",
      "Fold 8, Epoch 18, Val Loss: 0.011811395144395543\n",
      "Fold 8, Epoch 19, Val Loss: 0.012472709902546987\n",
      "Fold 8, Epoch 20, Val Loss: 0.012225522796257205\n",
      "Fold 8, Epoch 21, Val Loss: 0.01218436798829068\n",
      "Fold 8, Epoch 22, Val Loss: 0.013214214770454287\n",
      "Fold 8, Epoch 23, Val Loss: 0.013019549013423866\n",
      "Fold 8, Epoch 24, Val Loss: 0.015054161574736017\n",
      "Fold 8, Epoch 25, Val Loss: 0.014324427113050124\n",
      "Fold 8, Epoch 26, Val Loss: 0.013455132647694963\n",
      "Fold 8, Epoch 27, Val Loss: 0.013864714243688715\n",
      "Fold 8, Epoch 28, Val Loss: 0.01468874578214044\n",
      "Fold 8, Epoch 29, Val Loss: 0.015142743318392604\n",
      "Fold 8, Epoch 30, Val Loss: 0.014649702630239599\n",
      "Fold 8, Epoch 31, Val Loss: 0.015854022327754468\n",
      "Fold 8, Epoch 32, Val Loss: 0.015454996748882299\n",
      "Fold 8, Epoch 33, Val Loss: 0.01524892080479648\n",
      "Fold 8, Epoch 34, Val Loss: 0.016332884940745564\n",
      "Fold 8, Epoch 35, Val Loss: 0.014984875255033165\n",
      "Fold 8, Epoch 36, Val Loss: 0.017390927378443166\n",
      "Fold 8, Epoch 37, Val Loss: 0.016769934242223495\n",
      "Fold 8, Epoch 38, Val Loss: 0.01782080889280624\n",
      "Fold 8, Epoch 39, Val Loss: 0.017066622269753265\n",
      "Fold 8, Epoch 40, Val Loss: 0.017028967828546824\n",
      "Fold 8, Epoch 41, Val Loss: 0.018204492939482654\n",
      "Fold 8, Epoch 42, Val Loss: 0.016729597889769302\n",
      "Fold 8, Epoch 43, Val Loss: 0.018255310951603446\n",
      "Fold 8, Epoch 44, Val Loss: 0.018745806067272497\n",
      "Fold 8, Epoch 45, Val Loss: 0.01931689102277997\n",
      "Fold 8, Epoch 46, Val Loss: 0.019048889988526566\n",
      "Fold 8, Epoch 47, Val Loss: 0.01962046934938989\n",
      "Fold 8, Epoch 48, Val Loss: 0.018906926144816018\n",
      "Fold 8, Epoch 49, Val Loss: 0.021340245872786247\n",
      "Fold 8, Epoch 50, Val Loss: 0.02062358771799857\n",
      "Fold 9, Epoch 1, Val Loss: 0.005410281174352927\n",
      "Fold 9, Epoch 2, Val Loss: 0.005886174682850445\n",
      "Fold 9, Epoch 3, Val Loss: 0.0062971560505089156\n",
      "Fold 9, Epoch 4, Val Loss: 0.006648966954178997\n",
      "Fold 9, Epoch 5, Val Loss: 0.006942428545450887\n",
      "Fold 9, Epoch 6, Val Loss: 0.007435049360488868\n",
      "Fold 9, Epoch 7, Val Loss: 0.007723171263106368\n",
      "Fold 9, Epoch 8, Val Loss: 0.007988509538728811\n",
      "Fold 9, Epoch 9, Val Loss: 0.008258960374488115\n",
      "Fold 9, Epoch 10, Val Loss: 0.008710366090140682\n",
      "Fold 9, Epoch 11, Val Loss: 0.008797083423401504\n",
      "Fold 9, Epoch 12, Val Loss: 0.009283483171825228\n",
      "Fold 9, Epoch 13, Val Loss: 0.00949829416605429\n",
      "Fold 9, Epoch 14, Val Loss: 0.009667552893419766\n",
      "Fold 9, Epoch 15, Val Loss: 0.009951415861268293\n",
      "Fold 9, Epoch 16, Val Loss: 0.010169762168469664\n",
      "Fold 9, Epoch 17, Val Loss: 0.010437223431633346\n",
      "Fold 9, Epoch 18, Val Loss: 0.010807978264991712\n",
      "Fold 9, Epoch 19, Val Loss: 0.01097285530650538\n",
      "Fold 9, Epoch 20, Val Loss: 0.011729240814309544\n",
      "Fold 9, Epoch 21, Val Loss: 0.01146838494840226\n",
      "Fold 9, Epoch 22, Val Loss: 0.012100027064641174\n",
      "Fold 9, Epoch 23, Val Loss: 0.012341688904515731\n",
      "Fold 9, Epoch 24, Val Loss: 0.012299180674207573\n",
      "Fold 9, Epoch 25, Val Loss: 0.012545572157488426\n",
      "Fold 9, Epoch 26, Val Loss: 0.01299815212836496\n",
      "Fold 9, Epoch 27, Val Loss: 0.013257065116290918\n",
      "Fold 9, Epoch 28, Val Loss: 0.01342610117208923\n",
      "Fold 9, Epoch 29, Val Loss: 0.013701691244849908\n",
      "Fold 9, Epoch 30, Val Loss: 0.014127684322643703\n",
      "Fold 9, Epoch 31, Val Loss: 0.014385418239814126\n",
      "Fold 9, Epoch 32, Val Loss: 0.014535986417017326\n",
      "Fold 9, Epoch 33, Val Loss: 0.014545646318600286\n",
      "Fold 9, Epoch 34, Val Loss: 0.01495201669126049\n",
      "Fold 9, Epoch 35, Val Loss: 0.014941466300363325\n",
      "Fold 9, Epoch 36, Val Loss: 0.015176145232213788\n",
      "Fold 9, Epoch 37, Val Loss: 0.01564379354272076\n",
      "Fold 9, Epoch 38, Val Loss: 0.015512994236153398\n",
      "Fold 9, Epoch 39, Val Loss: 0.015814708899886846\n",
      "Fold 9, Epoch 40, Val Loss: 0.01588724997236568\n",
      "Fold 9, Epoch 41, Val Loss: 0.0160223211861111\n",
      "Fold 9, Epoch 42, Val Loss: 0.01635579903830314\n",
      "Fold 9, Epoch 43, Val Loss: 0.016726467322851848\n",
      "Fold 9, Epoch 44, Val Loss: 0.017079394048814513\n",
      "Fold 9, Epoch 45, Val Loss: 0.017167580016121065\n",
      "Fold 9, Epoch 46, Val Loss: 0.017252361152217913\n",
      "Fold 9, Epoch 47, Val Loss: 0.01773801180646125\n",
      "Fold 9, Epoch 48, Val Loss: 0.017791082850499605\n",
      "Fold 9, Epoch 49, Val Loss: 0.017626158955961688\n",
      "Fold 9, Epoch 50, Val Loss: 0.018097544496661365\n",
      "Fold 10, Epoch 1, Val Loss: 0.004020843448627561\n",
      "Fold 10, Epoch 2, Val Loss: 0.004058640704671616\n",
      "Fold 10, Epoch 3, Val Loss: 0.004495988024912508\n",
      "Fold 10, Epoch 4, Val Loss: 0.0047140237672196736\n",
      "Fold 10, Epoch 5, Val Loss: 0.005104916126335659\n",
      "Fold 10, Epoch 6, Val Loss: 0.005241306275407128\n",
      "Fold 10, Epoch 7, Val Loss: 0.005534283931328074\n",
      "Fold 10, Epoch 8, Val Loss: 0.006074859776917533\n",
      "Fold 10, Epoch 9, Val Loss: 0.006177092412590083\n",
      "Fold 10, Epoch 10, Val Loss: 0.006343735178275397\n",
      "Fold 10, Epoch 11, Val Loss: 0.006739036848392227\n",
      "Fold 10, Epoch 12, Val Loss: 0.006930723847525012\n",
      "Fold 10, Epoch 13, Val Loss: 0.007090221611903929\n",
      "Fold 10, Epoch 14, Val Loss: 0.007161488802094854\n",
      "Fold 10, Epoch 15, Val Loss: 0.007381070226446437\n",
      "Fold 10, Epoch 16, Val Loss: 0.0077279267331482\n",
      "Fold 10, Epoch 17, Val Loss: 0.00814912088864875\n",
      "Fold 10, Epoch 18, Val Loss: 0.008242328917602166\n",
      "Fold 10, Epoch 19, Val Loss: 0.008445746400391065\n",
      "Fold 10, Epoch 20, Val Loss: 0.008432879637770053\n",
      "Fold 10, Epoch 21, Val Loss: 0.008649346300337223\n",
      "Fold 10, Epoch 22, Val Loss: 0.009107309533460879\n",
      "Fold 10, Epoch 23, Val Loss: 0.00911696932501565\n",
      "Fold 10, Epoch 24, Val Loss: 0.009307295218248286\n",
      "Fold 10, Epoch 25, Val Loss: 0.009582578839394335\n",
      "Fold 10, Epoch 26, Val Loss: 0.009826920787428538\n",
      "Fold 10, Epoch 27, Val Loss: 0.010092981298583936\n",
      "Fold 10, Epoch 28, Val Loss: 0.0102208288442088\n",
      "Fold 10, Epoch 29, Val Loss: 0.010591419256099048\n",
      "Fold 10, Epoch 30, Val Loss: 0.010737909631093213\n",
      "Fold 10, Epoch 31, Val Loss: 0.010963187493664671\n",
      "Fold 10, Epoch 32, Val Loss: 0.011036398926576525\n",
      "Fold 10, Epoch 33, Val Loss: 0.011206033689714704\n",
      "Fold 10, Epoch 34, Val Loss: 0.011611595645423668\n",
      "Fold 10, Epoch 35, Val Loss: 0.011633651864454723\n",
      "Fold 10, Epoch 36, Val Loss: 0.011835895939803075\n",
      "Fold 10, Epoch 37, Val Loss: 0.011931141111991844\n",
      "Fold 10, Epoch 38, Val Loss: 0.01223787975394185\n",
      "Fold 10, Epoch 39, Val Loss: 0.01246482662786651\n",
      "Fold 10, Epoch 40, Val Loss: 0.01250586152253742\n",
      "Fold 10, Epoch 41, Val Loss: 0.012876527609865785\n",
      "Fold 10, Epoch 42, Val Loss: 0.012958141292450875\n",
      "Fold 10, Epoch 43, Val Loss: 0.013132647399645108\n",
      "Fold 10, Epoch 44, Val Loss: 0.01350398676677597\n",
      "Fold 10, Epoch 45, Val Loss: 0.013353697176460793\n",
      "Fold 10, Epoch 46, Val Loss: 0.013587195202648223\n",
      "Fold 10, Epoch 47, Val Loss: 0.013729660133171541\n",
      "Fold 10, Epoch 48, Val Loss: 0.013852823808618226\n",
      "Fold 10, Epoch 49, Val Loss: 0.014207539178773487\n",
      "Fold 10, Epoch 50, Val Loss: 0.01436432979629258\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define your SBERT model with additional dense layer\n",
    "model_name = \"sentence-transformers/xlm-r-distilroberta-base-paraphrase-v1\"  \n",
    "pooling_mode = \"mean\" \n",
    "input_dim = 768  \n",
    "model = Sent_Embed_Dense(model_name, pooling_mode, input_dim)\n",
    "\n",
    "model.to('cuda')\n",
    "tokenizer_ = model.sbert.tokenizer \n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_excel(r\"full data-EA-preprocessed.xlsx\" )\n",
    "\n",
    "data['Avg_EA'] = data[\"Average EA\"]/2\n",
    "User= data[\"Inferences\"]\n",
    "Des= data[\"Users' thoughts or feeling\"]\n",
    "User = User.str.lower().str.replace(':','').str.replace('i was', '').str.replace('she / he was', '').str.replace('s/he was', '').str.replace('she was', '').str.replace('he was', '').str.replace('they were', '')\n",
    "Des = Des.str.lower().str.replace(':','').str.replace('i was', '').str.replace('she / he was', '').str.replace('s/he was', '').str.replace('she was', '').str.replace('he was', '').str.replace('they were', '')\n",
    "\n",
    "data['user_thoughts_and _feelings'] = User\n",
    "data['designer_guess'] = Des\n",
    "data['Avg_EA'] = data[\"Average EA\"] / 2\n",
    "\n",
    "k_folds = 10\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=123)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "results = []\n",
    "\n",
    "All_pred = []\n",
    "Train_predictions = []\n",
    "Val_predictions = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(data)):\n",
    "    train_df = data.iloc[train_idx]\n",
    "    val_df = data.iloc[val_idx]\n",
    "    \n",
    "    train_dataloader = make_dataloader(train_df,  batch_size=1)\n",
    "    val_dataloader = make_dataloader(val_df, batch_size=1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        all_predictions_train = []\n",
    "        all_labels_train = []\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            inputs, labels = batch\n",
    "            inputs = {k: v for k, v in inputs.items()}  # Move inputs to GPU\n",
    "            labels = torch.stack(labels).to('cuda')\n",
    "            labels = labels.view(-1, 1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            all_predictions_train.append(outputs)\n",
    "            all_labels_train.append(labels)\n",
    "\n",
    "       \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_pearson, train_spearman, train_rmse = calculate_correlation(all_predictions_train, all_labels_train)\n",
    "        Train_predictions.append([train_pearson, train_spearman, train_rmse ])\n",
    "        #print(f\"Fold {fold + 1}, Epoch {epoch + 1}, Val Loss: {val_loss / len(val_dataloader)}\")\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                inputs, labels = batch\n",
    "                inputs = {k: v for k, v in inputs.items()}  # Move inputs to GPU\n",
    "                labels = torch.stack(labels).to('cuda')\n",
    "                labels = labels.view(-1, 1)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                all_predictions.append(outputs)\n",
    "                all_labels.append(labels)\n",
    "                \n",
    "                #print(len(all_predictions))\n",
    "                #print(all_predictions)\n",
    "            val_pearson,val_spearman, val_rmse  = calculate_correlation(all_predictions, all_labels)\n",
    "            Val_predictions.append([val_pearson,val_spearman, val_rmse])\n",
    "        print(f\"Fold {fold + 1}, Epoch {epoch + 1}, Val Loss: {val_loss / len(val_dataloader)}\")\n",
    "    \n",
    "    results.append({\n",
    "                'fold': fold,\n",
    "                'train_pearson': train_pearson, 'train_spearman': train_spearman,'train_rmse': train_rmse,\n",
    "                'val_pearson': val_pearson, 'val_spearman': val_spearman,'val_rmse': val_rmse,\n",
    "                #'baseline_pearson':baseline_pearson, 'baseline_spearman':baseline_spearman, 'baseline_rmse':baseline_rmse\n",
    "            })\n",
    "    All_pred.append({\n",
    "        'fold': fold,\n",
    "        'train_pearson': [i[0] for i in Train_predictions], 'train_spearman': [i[1] for i in Train_predictions],'train_rmse': [i[2] for i in Train_predictions],\n",
    "         'val_pearson': [i[0] for i in Val_predictions], 'val_spearman': [i[1] for i in Val_predictions],'val_rmse': [i[2] for i in Val_predictions],\n",
    "    \n",
    "    })\n",
    "    \n",
    "        \n",
    "# Save the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.to_csv(\"Dense_layer_experiment_with_SBERT Concatination evaluation.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5c4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32d677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
